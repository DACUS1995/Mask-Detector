{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using tensorflow version: 2.2.0\nUsing eager execution: True\n"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Using tensorflow version: \" + tf.__version__)\n",
    "print(\"Using eager execution: \" + str(tf.executing_eagerly())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "    while \"models\" in pathlib.Path.cwd().parts:\n",
    "        os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "    !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total number of samples: 680\nDataset training size: 544 | Dataset validation size: 136\n"
    }
   ],
   "source": [
    "DATASET_PATH = \"../dataset\"\n",
    "\n",
    "masked_faces_paths = list(pathlib.Path(DATASET_PATH + \"/masked_faces\").glob('*'))\n",
    "normal_faces_paths = list(pathlib.Path(DATASET_PATH + \"/normal_faces\").glob('*'))\n",
    "\n",
    "length = len(masked_faces_paths)\n",
    "print(f\"Total number of samples: {length}\")\n",
    "\n",
    "split_ratio = 0.8\n",
    "training_size = int(split_ratio * length)\n",
    "validation_size = length - training_size\n",
    "print(f\"Dataset training size: {training_size} | Dataset validation size: {validation_size}\")\n",
    "\n",
    "training_paths = masked_faces_paths[:training_size] + normal_faces_paths[:training_size]\n",
    "validation_paths = masked_faces_paths[training_size:] + normal_faces_paths[training_size:]\n",
    "\n",
    "targets = None\n",
    "with open(DATASET_PATH + \"/targets.json\") as json_file:\n",
    "\ttargets = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'masked_faces'"
     },
     "metadata": {},
     "execution_count": 16
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'bbox': [[194, 246, 656, 708]]}"
     },
     "metadata": {},
     "execution_count": 16
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1, 4)"
     },
     "metadata": {},
     "execution_count": 16
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x1fa20d08b88>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "image_path = training_paths[2]\n",
    "image_name = str(image_path).split(os.sep)[3]\n",
    "str(image_path).split(os.sep)[2]\n",
    "targets[image_name]\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "np.array(targets[image_name][\"bbox\"]).shape\n",
    "plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    num_classes = 2\n",
    "    label_id_offset = 1\n",
    "\n",
    "    train_image_tensors = []\n",
    "    gt_classes_one_hot_tensors = []\n",
    "    gt_box_tensors = []\n",
    "\n",
    "    for image_path in zip(training_paths):\n",
    "        train_image_np = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "        image_name = str(image_path).split(os.sep)[2]\n",
    "        gt_box_np = np.array(targets[image_name][\"bbox\"])\n",
    "\n",
    "        train_image_tensors.append(\n",
    "            tf.expand_dims(\n",
    "                tf.convert_to_tensor(\n",
    "                    train_image_np, dtype=tf.float32\n",
    "                ), axis=0)\n",
    "            )\n",
    "\n",
    "        gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
    "        zero_indexed_groundtruth_classes = tf.convert_to_tensor(\n",
    "            np.concatanate(\n",
    "                np.zeros(shape=[gt_box_np.shape[0] / 2], dtype=np.int32),\n",
    "                np.ones(shape=[gt_box_np.shape[0] / 2], dtype=np.int32),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        gt_classes_one_hot_tensors.append(\n",
    "            tf.one_hot(\n",
    "                zero_indexed_groundtruth_classes, num_classes\n",
    "            )\n",
    "        )\n",
    "    print('Done prepping data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    print('Building model and restoring weights for fine-tuning...', flush=True)\n",
    "    num_classes = 1\n",
    "    pipeline_config = 'models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
    "    checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
    "\n",
    "    configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "    model_config = configs['model']\n",
    "    model_config.ssd.num_classes = num_classes\n",
    "    model_config.ssd.freeze_batchnorm = True\n",
    "    detection_model = model_builder.build(model_config=model_config, is_training=True)\n",
    "\n",
    "    fake_box_predictor = tf.compat.v2.train.Checkpoint(\n",
    "        _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
    "        _box_prediction_head=detection_model._box_predictor._box_prediction_head\n",
    "    )\n",
    "\n",
    "    fake_model = tf.compat.v2.train.Checkpoint(\n",
    "        _feature_extractor=detection_model._feature_extractor,\n",
    "        _box_predictor=fake_box_predictor\n",
    "    )\n",
    "    ckpt = tf.compat.v2.train.Checkpoint(model=fake_model)\n",
    "    ckpt.restore(checkpoint_path).expect_partial()\n",
    "\n",
    "    image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    _ = detection_model.postprocess(prediction_dict, shapes)\n",
    "    print('Weights restored!')\n",
    "\n",
    "    return detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    model,\n",
    "    data,\n",
    "    labels,\n",
    "    optimizer,\n",
    "    clip_gradients_value = None\n",
    "):\n",
    "    with tf.GradientTape as tape:\n",
    "        losses_dict = compute_loss(model, data, labels)\n",
    "        total_loss = losses_dict[\"total_loss\"]\n",
    "\n",
    "    trainable_variables = model.trainable_variables\n",
    "    gradients = tape.gradients(total_loss, trainable_variables)\n",
    "\n",
    "    if clip_gradients_value is not None:\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, clip_gradients_value)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    \n",
    "    return total_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-ac4929b4bed2>, line 4)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-ac4929b4bed2>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    clip_gradients_value = if \"clip_gradients_value\" in config else None\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def train_loop(config):\n",
    "    epochs = config[\"epochs\"]\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    clip_gradients_value = config[\"clip_gradients_value\"] if \"clip_gradients_value\" in config else None\n",
    "    evaluation_interval = config[\"evaluation_interval\"] if \"evaluation_interval\" in  config else 1\n",
    "    metrics_interval = config[\"metrics_interval\"] if \"metrics_interval\" in config else 5\n",
    "\n",
    "    model = get_model()\n",
    "    training_dataset, validation_dataset = get_dataset()\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    train_losses = []\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "\n",
    "    print(\"Start training!\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for x, y in training_dataset:\n",
    "            total_loss = train_step(\n",
    "                model,\n",
    "                x, y,\n",
    "                optimizer,\n",
    "                clip_gradients_value\n",
    "            )\n",
    "\n",
    "            epoch_loss_avg(total_loss)\n",
    "\n",
    "        train_loss_results.append(epoch_loss_avg.result())\n",
    "        epoch_time_elapsed = time.time() - epoch_start_time\n",
    "\n",
    "\t\tif epoch % metrics_interval == 0:\n",
    "\t\t\tprint(\n",
    "                \"Epoch {:03d} | Loss: {:.3f} | Time: {:.0f}m {:.0f}s\"\n",
    "                .format(epoch, epoch_loss_avg.result(), time_elapsed // 60, time_elapsed % 60)\n",
    "            )\n",
    "\n",
    "        if epoch % evaluation_interval == 0:\n",
    "\t\t    evaluate_model(model, validation_dataset)\n",
    "\n",
    "        epoch_loss_avg.reset_states()\n",
    "    print(\"Training finished\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epochs\": 10,\n",
    "    \"learning_rate\": 0.001\n",
    "}\n",
    "\n",
    "trained_model =train_loop(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('tf': conda)",
   "display_name": "Python 3.7.9 64-bit ('tf': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dbd4edd9f398c84cb16feeeb2405d326207e0a74e0aaa6e730cda3070d5db784"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}